{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Notes on Data Structures and Algs\r\n",
    "\r\n",
    "This notebook has only brief notes concerning the topic. Just as a refresher when prepping for the coding interview.\r\n",
    "\r\n",
    "### 3 Key Terms\r\n",
    "1. Complexity Analysis:\r\n",
    "The process of determining how efficient an algorithm is. Complexity Analysis usually involves finding both the time complexity and the space complexity of an algorithm. Complexity Analysis is effectively used to determine how 'good' an algorithm is and whether it's 'better' than another one.\r\n",
    "\r\n",
    "2. Time complexity\r\n",
    "A measure of how fast an algorithm runs, time complexity is a central concept in the field of algorithms and in coding interviews. It's expressed using `*Big O*` Notation.\r\n",
    "\r\n",
    "3. Space Complexity\r\n",
    "A measure of how much auxiliary memory an algorithm takes up, complexity is a central concepts in the field of algorithms and in coding interviews. It's expressed using `*Big O*` Notation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Memory\r\n",
    "\r\n",
    "Whenever you execute any simple program like declaring a variable, that variable is stored in memory in which you can retreive later. Now you can think of memory as a canvas (just a grid, with different slots in your machine) where you can store this information.\r\n",
    "* Each grid has numbers in it (expressed as binary numbers) in which they are reffered as memory adresses.\r\n",
    "\r\n",
    "* Each memory slot can take only 8 bits (now known as bytes). And one memory slot can represent up to 2**8 = 256 number. So if we have data which has more value than 256 we then increase the number of bits from 8 bits to let's say 32 bits, or 64 bits and so on. Note: As we increase the number of bits, we also increase the number of memory slots we can accomodate on our data\r\n",
    "\r\n",
    "* And data is stored in this memory slots in terms of binary numbers also.\r\n",
    "\r\n",
    "* For the case of strings, we map the ASCII values of the strings to base ten numbers and then to base 2 (binary numbers)\r\n",
    "\r\n",
    "`Fixed-Width-Integer` -> An integer represented by a fixed amount of bits. For example, a 32-bit integer is an integer represented by 32 bits (4 bytes), and a 64-bit integer is an integer represented by 64 bits (8 bytes). Regardless of how large an integer is, its fixed-width-representation is, by defn, made up of a constant number of bits. It follows that, regardless of how large an integer is, an operation perfomed on its fixed width-integer representation consists of a constant number of bit manipulations, since the integer is made up of a fixed number of bits"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Big O Notation:\r\n",
    "\r\n",
    "New Defn of `Time Complexity`\r\n",
    "\r\n",
    "`Time Complexity` is the measure of how fast an algorithms is as the size of the input increases\r\n",
    "\r\n",
    "Variables used in Big O Notation denotes the sizes of inputs to algorithms. For example, O(n) might be the time complexity of an algorithm that traverses through an array of length m; similarlly, O(n + m) might be the time complexity of an algorithm that traverses through an array of length n and through of string of length m.\r\n",
    "\r\n",
    "The following are examples of common complexities and their Big O notations, ordered from fastest to slowest:\r\n",
    "1. Constant: O(1) :-> Read as the size of the input increases, the space-time complexity remains constant. The one represents the number of operations, but it doesn't make sense to put o(n), where n here represents a number (integer) because the number of operations (eg, accessing the data on the memory slots etc) doesn't have a connection to the size of the input. So in most cases we just use one and thus, O(1).\r\n",
    "\r\n",
    "2. Logarithmic: O(log(n)): -> read as the size of input increases to infinity, the space-time complexity of the algorithm is log(n) where n is the size of input.\r\n",
    "\r\n",
    "3. Linear: O(n)\r\n",
    "\r\n",
    "4. Log-linear: O(nlog(n))\r\n",
    "\r\n",
    "5. Quadratic: O(n**2): n square\r\n",
    "\r\n",
    "6. Cubic: O(n**3): n cubic\r\n",
    "\r\n",
    "7. Exponential: O(2**n): 2 to the power of n\r\n",
    "\r\n",
    "8. Factorial: O(n!)\r\n",
    "\r\n",
    "\r\n",
    "Note that in the context of coding interviews, Big O notation is usually understood to describe the worst-case complexity of an algorithm, wvwn though the worst-case complexity might differ from the average-case complexity.\r\n",
    "\r\n",
    "For example, some sorting algorithms have different time complexities depending on the layout of elements in their input array. In rare cases, their time complexity will be much worse than in more common cases. Similarly, an algorithm that takes in a string and perfomes special operationson uppercase characters might have a different time complexity when run on an input string of only uppercase characters vs on an input string with just a few uppercase characters.\r\n",
    "\r\n",
    "<br>\r\n",
    "Thus, when describing the time complexity of an algotithm, it can sometimes be helpful to specify whether the time complexity refers to the average case or the worst case (e.g, 'this algorithms runs in O(nlog(n))) time on average and in O(n**2) time in the worst case\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logarithm\r\n",
    "\r\n",
    "A mathematical concept that's widely used in Computer Science and that's defined by the following equation:\r\n",
    "\r\n",
    "log of x to the base of b equals to y if and only if b to the power of y = x\r\n",
    "\r\n",
    "log <sub>b</sub> (x) = y if and only if b<sub>y</sub> = x\r\n",
    "\r\n",
    "In the context of coding interviews, the logarithms is used to describe the complexity analysis pf algorithms, and its usage always implies a logarithm of base 2. In other words, the logarithm used in the context of coding interviews is defined by the following equation.\r\n",
    "\r\n",
    "log(n) = y if and only if 2<sub>y</sub> = n\r\n",
    "\r\n",
    "In plain english, if an algortihm has a logarithmic time complexity (Olog(n)), where n is the size of the input), then whenever the algorithm's input doubles in size (i.e whenever n doubles), the number of operationsneeded to complete the algorithm oly increases by one unit. Conversely, an algorithm with a linear time complexity would see its number of operations double if its input size doubled.\r\n",
    "\r\n",
    "As an example, a linear-time-complexity algorithm with an input of size 1,000 might take roughly 1,000 operations to complete, whereas a logarithmic-time-complexity algorithm with the same input would taje roughly 10 operations to complete, since 2<sup>10</sup> ~= 1,000"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Arrays\r\n",
    "\r\n",
    "Arrays are one of the basic concepts in computer programming, long story short; they are reffered to as lists in python.\r\n",
    "\r\n",
    "There are two types of arrays:\r\n",
    "* Static Arrays: -> This is the kind of arrays in which you specify the length of the array, so that the OS can know how many memory slots can be accessed so that they can be stored. \r\n",
    "* Dynamic Arrays: ->\r\n",
    "\r\n",
    "\r\n",
    "Different operations on Arrays and their Big O Notation.\r\n",
    "1. Accessing elements in an array: `O(1) Constant time-complexity`\r\n",
    "    This is because the OS knows exactly how many memory slots that a particular array has used, and so when you index a particular item from an array say array[2], what happens is that it's going to multply 8*2(since each integer is a fixed-width integer) and it will go to the 16<sub>th</sub> memory slot and there the item will be located.\r\n",
    "\r\n",
    "2. Overwriting an element in an array: `O(1) Constant time-complexity`.\r\n",
    "    The same explanation applies here, since it's known where exactly a particular item is stored in a memory slot.\r\n",
    "\r\n",
    "3. Initializing an array: `O(n) linear time-complexity`.\r\n",
    "    This is because the number of memory slots allocated for storing the array increases as the size of input n increases\r\n",
    "\r\n",
    "4. Traversing through an array (e.g for loop, mapping an array, while loops e.t.c): `O(n) linear time-complexity` on time complexity and `O(1) complexity` on space\r\n",
    "    Because we're not using any extra space, and not doing anything else just traversing, it's going to be O(1), but when it comes to time it's going to be O(n) because you're traversing through n items\r\n",
    "\r\n",
    "5. Copy `O(n) space-time complexity`\r\n",
    "     Because you're traversing throught an array of lenghth n and then create another copy of the length n.\r\n",
    "\r\n",
    "6. Inserting a value to an array: `O(1) space-complexity` and `O(n) time complexity`.\r\n",
    "     The above complexities applies only for static arrays. for dynamic arrays, it allows to have faster insertion of items in an array, because it allocates twice the ammount of memory slots as the size of your array. For that matter it uses `constant-time-complexity`. So let's say you start with an array of length 1 say array= [1], the OS will allocate the memory slot of size 2, so that you can have faster insertion. Once you append a third item, it's going to have `O(n) complexity` because it has to copy itself and now it will give you a total of six memory slots, and so on. \r\n",
    "\r\n",
    "     For Dynamic array: `O(1) Time-complexity` \r\n",
    "\r\n",
    "     For Static arrays: `O(n) Complexity`\r\n",
    "\r\n",
    "7. Removing an element at the beginning: It's `O(n) ST complexity`\r\n",
    "\r\n",
    "8. Removing a value at the end: `O(1) ST complexity`\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Singly Linked List\r\n",
    "A data structure that consists of nodes, each with some value and a pointer to the next node in the linked list. A linked list node's value and next node are typically stored in `value` and `next` properties, respectively.\r\n",
    "\r\n",
    "The first node in a linked list is reffered to as the head of the linked list, whose `next` properties points to the `null` value, is known as the tail of the linked list.\r\n",
    "\r\n",
    "Below is a visual representation of a singly linked list whose nodes hold integer values:\r\n",
    "\r\n",
    "`0 -> 1 -> 2 -> 3 -> null`\r\n",
    "\r\n",
    "A singly linked list typically exposes its head to its user for easy access. While finding a node in a singly linked list involves traversing through all of the nodes leading up to the node in question (as opposed to instant access with an array), adding or removing nodes simply involves overwriting `next` pointers (assuming that you have access to the node right before the node that you're adding or removing).\r\n",
    "\r\n",
    "The following are a singly linked list's standard operations and their corresponding time complexities.\r\n",
    "\r\n",
    "* Accessing the head: O(1)\r\n",
    "* Accessing the tail: O(n)\r\n",
    "* Accessing a middle node: O(n)\r\n",
    "* Inserting/removing the head: O(1)\r\n",
    "* Inserting/removing the tail: O(n) to access + O(1) to insert\r\n",
    "* Inserting/removing a middle node: O(n) to access + O(1) to insert\r\n",
    "* Searching for a value: O(n)\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Doubly Linked list\r\n",
    "Similar to singly linked list, except that each node in a doubly linked list also has a pointer to the previous node in the linked list. The previous node is typically stores in a `prev` property.\r\n",
    "\r\n",
    "Just as the `next` property of a doubly linked list's tail points to the `null` value, so too does the `prev` property of a DLL list's head.\r\n",
    "\r\n",
    "\r\n",
    "Below is a visual representation of a doubly linked list whose nodes hold integer values:\r\n",
    "\r\n",
    "`null <- 0 <-> 1 <-> 2 <-> 3 -> null`\r\n",
    "\r\n",
    "While a doubly linked list typically exposes both its head and tail to its user, as opposed to just uts head in the case of a SLL, it otherwise behaves very similarly to a SLL.\r\n",
    "\r\n",
    "The following are a doubly linked list's standard operation and their corresponding complexities:\r\n",
    "\r\n",
    "* Accessing the head: O(1)\r\n",
    "* Accessing the tail: O(1)\r\n",
    "* Accessing the node: O(n)\r\n",
    "* Inserting/removing the head: O(1)\r\n",
    "* Inserting/removing the tail: O(1)\r\n",
    "* Inserting/removing the middle node: O(n) to access + O(1)\r\n",
    "* Searching for a value: O(n)\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Circular Linked List\r\n",
    "A linked list that has no clear head or tail, because its \"tail points to its 'head' effectively forming a closed circle.\r\n",
    "\r\n",
    "A circular linked list can be either a **singly circular linked list** or a **doubly circular linked list.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hash Tables\r\n",
    "A data structure that provides fast insertion, deletion, and lookup of key/value pairs.\r\n",
    "\r\n",
    "Under the hood, a hash table uses a **dynamic array** of **linked list** to efficiently store key/value pairs. When inseting a key/value pair, a hasg function first maps the key, which is typically a string (or any data that can be hashed, depending on the implementation of the hash table), to an integer value and, by extension, to an index in the underlying dynamic array. Then, the value associated with the key is added to the linked list stored at that index in the dynamic array, and a reference to the key is also stored with the value.\r\n",
    "\r\n",
    "Hash tables rely on highly optimized hash functions to minimize the number of **collisions** that occue when storing values: cases where two keys map to the same index.\r\n",
    "\r\n",
    "Below is an example of what a hash table might look like under the hood:\r\n",
    "\r\n",
    "[\r\n",
    "    0: (value1, key1) -> null\r\n",
    "    1: (value2, key2) -> (value3, key3) -> (value4, key4)\r\n",
    "    2: null\r\n",
    "    3: (value5, key5) -> (value6, key6)\r\n",
    "    4: (value7, key7) -> null\r\n",
    "]\r\n",
    "\r\n",
    "In the hash table above, the keys **key2, key3** collided by all being hashed to 1, the same applies to **key5 and key6**\r\n",
    "\r\n",
    "The following are a hash's table's standard operations and their corresponding time complexities:\r\n",
    "* Inserting a key/value pair: O(1) on average, O(n) in the worst case\r\n",
    "* Removing a key/value pair: O(1) on average, O(n) in the worst case\r\n",
    "* Looking up a key: O(1) on average, O(n) in the worst case\r\n",
    "\r\n",
    "The worst-case linear-time operations occur when a hash table experiences a lot of collisions, leading to long linked lists internally, which take O(n) time to traverse.\r\n",
    "\r\n",
    "However, in practice and especially in coding interviews, we typically assume that the hash functions employed by hash tab;es are so optimized that collisions are extremely rare and constant-time operations are guranteed"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stacks And Queues\r\n",
    "Push. Pop. FIFO. LIFO. That pretty much sums up stacks adn queues.\r\n",
    "\r\n",
    "`Stack` -> An array-like data structure whose elements follow the LIFO rule: Last In First Out.\r\n",
    "\r\n",
    "A stack is often compared to a stack of books on a table: the last book that's places on the stack is the first one that's taken off the stack.\r\n",
    "\r\n",
    "The following are a stack's standard operations and their corresponding time complexities.\r\n",
    "\r\n",
    "* Pushing an element onto the stack: O(1)\r\n",
    "* Popping an element off the stack O(1)\r\n",
    "* Peeking at the element in the top of the stack: O(1)\r\n",
    "* Searching for an element:  O(n)\r\n",
    "\r\n",
    "A stack is typically implemented with a dynamic array or with a sigly linked list.\r\n",
    "\r\n",
    "## Queue\r\n",
    "An array-like data structure whose elements follow the FIFO rule: First In First Out.\r\n",
    "\r\n",
    "A queue is often compared to a group of people standing in line to purchase items at a store: the first person to get in line is the first one to purchase items and to get out of the queue.\r\n",
    "\r\n",
    "The following are a queue's standard operations and their corresponding time complexities:\r\n",
    "* Enqueuing an element into the queue: O(1)\r\n",
    "* Dequeueing an element out of the queue: O(1)\r\n",
    "* Peeking at the element at the front of the queue: O(1)\r\n",
    "* Searching for an element in the queue: O(n)\r\n",
    " A queue is typically implemented with a doubly linked list\r\n",
    " "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('Tony_ml': conda)"
  },
  "interpreter": {
   "hash": "3cda1abfc2264e5aafed419a9d1c945a6bcbd7c77187ca59a7878bb80eaea557"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}